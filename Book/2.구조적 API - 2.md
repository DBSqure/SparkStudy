---
tags:
  - "#Spark"
created: 2024-05-05
---


## 1. 데이터 타입 다루기

### A. 데이터 타입 변환

- `lit` 함수를 사용해 데이터 타입 변환
	- 다른 언어의 데이터 타입을 스파크 데이터 타입에 맞게 변환

```python
from pyspark.sql.functions import lit

df.select(lit(5), lit("five"), lit(5.0))
```
```
Out[4]: DataFrame[5: int, five: string, 5.0: double]
```


### B. boolean 데이터 타입

- true, false, and, or을 활용해 필터링
```python
from pyspark.sql.functions import col
from pyspark.sql.functions import instr

priceFilter = col("UnitPrice") > 600
descripFilter = instr(df.Description, "POSTAGE") >= 1
df.where(df.StockCode.isin("DOT")).where(priceFilter | descripFilter).show()
```
```
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|
|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
```

- boolean 컬럼을 활용해 DF 필터링
```python
from pyspark.sql.functions import col
from pyspark.sql.functions import instr

DOTCodeFilter = col("StockCode") == "DOT"
priceFilter = col("UnitPrice") > 600
descripFilter = instr(col("Description"), "POSTAGE") >= 1
df.withColumn("isExpensive", DOTCodeFilter & (priceFilter | descripFilter))\
  .where("isExpensive")\
  .select("unitPrice", "isExpensive").show(5)
```

- null 값 데이터 확인을 위해 null 값에 안전한지 동치 테스트 
```python
df.where(col("Description").eqNullSafe("hello")).show()
```


### C. 수치형 데이터 타입

- 일반적으로 연산 방식을 정의하는 형태로 사용
```python
from pyspark.sql.functions import expr, pow

new_num = pow(col("Quantity") * col("UnitPrice"), 2) + 5
df.select(expr("CustomerID"), new_num.alias("realQuantity")).show(2)
```
```
+----------+------------------+
|CustomerID|      realQuantity|
+----------+------------------+
|   17850.0|239.08999999999997|
|   17850.0|          418.7156|
+----------+------------------+
```

#### 반올림, 내림
- 반올림 `round`, 내림 `bround`
```python
from pyspark.sql.functions import lit, round, bround

df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)
```

#### 고유 번호 붙이기
```python
from pyspark.sql.functions import monotonically_increasing_id

df.select(monotonically_increasing_id()).show(2)
```
```
+-----------------------------+
|monotonically_increasing_id()|
+-----------------------------+
|                            0|
|                            1|
+-----------------------------+
```


### D. 문자열 데이터 타입

- `initcap` : 문자열에서 공백으로 나눠진 모든 단어의 첫글자 대분자로 변경
```python
from pyspark.sql.functions import col
from pyspark.sql.functions import initcap

df.select(initcap(col("Description"))).show(2)
```
```
+--------------------+
|initcap(Description)|
+--------------------+
|White Hanging Hea...|
| White Metal Lantern|
+--------------------+
```

- `upper`, `lower` : 대소문자 변경
```python
from pyspark.sql.functions import lower, upper

df.select(col("Description"),
    lower(col("Description")),
    upper(lower(col("Description")))).show(2)
```
```
+--------------------+--------------------+-------------------------+
|         Description|  lower(Description)|upper(lower(Description))|
+--------------------+--------------------+-------------------------+
|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|
| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|
+--------------------+--------------------+-------------------------+
```

- `trim` : `ltrim` 왼쪽 공백, `rtrim` 오른쪽 공백, `trim` 양옆 공백
- `pad` : pad 크기만큼 문자열 수정
```python
from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim

df.select(
    ltrim(lit("    HELLO    ")).alias("ltrim"),
    rtrim(lit("    HELLO    ")).alias("rtrim"),
    trim(lit("    HELLO    ")).alias("trim"),
    lpad(lit("HELLO"), 3, " ").alias("lp"),
    rpad(lit("HELLO"), 10, " ").alias("rp")).show(2)
```
```
+---------+---------+-----+---+----------+
|    ltrim|    rtrim| trim| lp|        rp|
+---------+---------+-----+---+----------+
|HELLO    |    HELLO|HELLO|HEL|HELLO     |
|HELLO    |    HELLO|HELLO|HEL|HELLO     |
+---------+---------+-----+---+----------+
```

- .`regexp_replace` : 정규표현식 치환
```python
from pyspark.sql.functions import regexp_replace

regex_string = "BLACK|WHITE|RED|GREEN|BLUE"
df.select(
  regexp_replace(col("Description"), regex_string, "COLOR").alias("color_clean"),
  col("Description")).show(2)
```
```
+--------------------+--------------------+
|         color_clean|         Description|
+--------------------+--------------------+
|COLOR HANGING HEA...|WHITE HANGING HEA...|
| COLOR METAL LANTERN| WHITE METAL LANTERN|
+--------------------+--------------------+
```

- `translate` : 문자 치환
```python
from pyspark.sql.functions import translate

df.select(translate(col("Description"), "LEET", "1337"),col("Description"))\
  .show(2)
```
```
+----------------------------------+--------------------+
|translate(Description, LEET, 1337)|         Description|
+----------------------------------+--------------------+
|              WHI73 HANGING H3A...|WHITE HANGING HEA...|
|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|
+----------------------------------+--------------------+
```

- `regexp_extract` : 문자 추출
```python
from pyspark.sql.functions import regexp_extract

extract_str = "(BLACK|WHITE|RED|GREEN|BLUE)"
df.select(
     regexp_extract(col("Description"), extract_str, 1).alias("color_clean"),
     col("Description")).show(2)
```
```
+-----------+--------------------+
|color_clean|         Description|
+-----------+--------------------+
|      WHITE|WHITE HANGING HEA...|
|      WHITE| WHITE METAL LANTERN|
+-----------+--------------------+
```

- `instr` : 문자 있는지 확인
```python
from pyspark.sql.functions import instr

containsBlack = instr(col("Description"), "BLACK") >= 1
containsWhite = instr(col("Description"), "WHITE") >= 1
df.withColumn("hasSimpleColor", containsBlack | containsWhite)\
  .where("hasSimpleColor")\
  .select("Description").show(3, False)
```
```
+----------------------------------+
|Description                       |
+----------------------------------+
|WHITE HANGING HEART T-LIGHT HOLDER|
|WHITE METAL LANTERN               |
|RED WOOLLY HOTTIE WHITE HEART.    |
+----------------------------------+
```

- `locate` : 문자열 위치(1부터 시작)를 정수로 반환
```python
from pyspark.sql.functions import expr, locate

simpleColors = ["black", "white", "red", "green", "blue"]
def color_locator(column, color_string):
  return locate(color_string.upper(), column)\
          .cast("boolean")\
          .alias("is_" + color_string)
selectedColumns = [color_locator(df.Description, c) for c in simpleColors]
selectedColumns.append(expr("*")) # Column 타입

df.select(*selectedColumns).where(expr("is_white OR is_red"))\
  .select("Description").show(3, False)
```
```
+----------------------------------+
|Description                       |
+----------------------------------+
|WHITE HANGING HEART T-LIGHT HOLDER|
|WHITE METAL LANTERN               |
|RED WOOLLY HOTTIE WHITE HEART.    |
+----------------------------------+
```




