---
tags:
  - "#Spark"
created: 2024-05-05
---


## 1. 데이터 타입 다루기

### A. 데이터 타입 변환

- `lit` 함수를 사용해 데이터 타입 변환
	- 다른 언어의 데이터 타입을 스파크 데이터 타입에 맞게 변환

```python
from pyspark.sql.functions import lit

df.select(lit(5), lit("five"), lit(5.0))
```
```
Out[4]: DataFrame[5: int, five: string, 5.0: double]
```


### B. boolean 데이터 타입

- true, false, and, or을 활용해 필터링
```python
from pyspark.sql.functions import col
from pyspark.sql.functions import instr

priceFilter = col("UnitPrice") > 600
descripFilter = instr(df.Description, "POSTAGE") >= 1
df.where(df.StockCode.isin("DOT")).where(priceFilter | descripFilter).show()
```
```
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|
|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|
+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
```

- boolean 컬럼을 활용해 DF 필터링
```python
from pyspark.sql.functions import col
from pyspark.sql.functions import instr

DOTCodeFilter = col("StockCode") == "DOT"
priceFilter = col("UnitPrice") > 600
descripFilter = instr(col("Description"), "POSTAGE") >= 1
df.withColumn("isExpensive", DOTCodeFilter & (priceFilter | descripFilter))\
  .where("isExpensive")\
  .select("unitPrice", "isExpensive").show(5)
```

- null 값 데이터 확인을 위해 null 값에 안전한지 동치 테스트 
```python
df.where(col("Description").eqNullSafe("hello")).show()
```


### C. 수치형 데이터 타입

- 일반적으로 연산 방식을 정의하는 형태로 사용
```python
from pyspark.sql.functions import expr, pow

new_num = pow(col("Quantity") * col("UnitPrice"), 2) + 5
df.select(expr("CustomerID"), new_num.alias("realQuantity")).show(2)
```
```
+----------+------------------+
|CustomerID|      realQuantity|
+----------+------------------+
|   17850.0|239.08999999999997|
|   17850.0|          418.7156|
+----------+------------------+
```

#### 반올림, 내림
- 반올림 `round`, 내림 `bround`
```python
from pyspark.sql.functions import lit, round, bround

df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)
```

#### 고유 번호 붙이기
```python
from pyspark.sql.functions import monotonically_increasing_id

df.select(monotonically_increasing_id()).show(2)
```
```
+-----------------------------+
|monotonically_increasing_id()|
+-----------------------------+
|                            0|
|                            1|
+-----------------------------+
```


### D. 문자열 데이터 타입

- `initcap` : 문자열에서 공백으로 나눠진 모든 단어의 첫글자 대분자로 변경
```python
from pyspark.sql.functions import col
from pyspark.sql.functions import initcap

df.select(initcap(col("Description"))).show(2)
```
```
+--------------------+
|initcap(Description)|
+--------------------+
|White Hanging Hea...|
| White Metal Lantern|
+--------------------+
```

- `upper`, `lower` : 대소문자 변경
```python
from pyspark.sql.functions import lower, upper

df.select(col("Description"),
    lower(col("Description")),
    upper(lower(col("Description")))).show(2)
```
```
+--------------------+--------------------+-------------------------+
|         Description|  lower(Description)|upper(lower(Description))|
+--------------------+--------------------+-------------------------+
|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|
| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|
+--------------------+--------------------+-------------------------+
```

- `trim` : `ltrim` 왼쪽 공백, `rtrim` 오른쪽 공백, `trim` 양옆 공백
- `pad` : pad 크기만큼 문자열 수정
```python
from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim

df.select(
    ltrim(lit("    HELLO    ")).alias("ltrim"),
    rtrim(lit("    HELLO    ")).alias("rtrim"),
    trim(lit("    HELLO    ")).alias("trim"),
    lpad(lit("HELLO"), 3, " ").alias("lp"),
    rpad(lit("HELLO"), 10, " ").alias("rp")).show(2)
```
```
+---------+---------+-----+---+----------+
|    ltrim|    rtrim| trim| lp|        rp|
+---------+---------+-----+---+----------+
|HELLO    |    HELLO|HELLO|HEL|HELLO     |
|HELLO    |    HELLO|HELLO|HEL|HELLO     |
+---------+---------+-----+---+----------+
```

- .`regexp_replace` : 정규표현식 치환
```python
from pyspark.sql.functions import regexp_replace

regex_string = "BLACK|WHITE|RED|GREEN|BLUE"
df.select(
  regexp_replace(col("Description"), regex_string, "COLOR").alias("color_clean"),
  col("Description")).show(2)
```
```
+--------------------+--------------------+
|         color_clean|         Description|
+--------------------+--------------------+
|COLOR HANGING HEA...|WHITE HANGING HEA...|
| COLOR METAL LANTERN| WHITE METAL LANTERN|
+--------------------+--------------------+
```

- `translate` : 문자 치환
```python
from pyspark.sql.functions import translate

df.select(translate(col("Description"), "LEET", "1337"),col("Description"))\
  .show(2)
```
```
+----------------------------------+--------------------+
|translate(Description, LEET, 1337)|         Description|
+----------------------------------+--------------------+
|              WHI73 HANGING H3A...|WHITE HANGING HEA...|
|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|
+----------------------------------+--------------------+
```

- `regexp_extract` : 문자 추출
```python
from pyspark.sql.functions import regexp_extract

extract_str = "(BLACK|WHITE|RED|GREEN|BLUE)"
df.select(
     regexp_extract(col("Description"), extract_str, 1).alias("color_clean"),
     col("Description")).show(2)
```
```
+-----------+--------------------+
|color_clean|         Description|
+-----------+--------------------+
|      WHITE|WHITE HANGING HEA...|
|      WHITE| WHITE METAL LANTERN|
+-----------+--------------------+
```

- `instr` : 문자 있는지 확인
```python
from pyspark.sql.functions import instr

containsBlack = instr(col("Description"), "BLACK") >= 1
containsWhite = instr(col("Description"), "WHITE") >= 1
df.withColumn("hasSimpleColor", containsBlack | containsWhite)\
  .where("hasSimpleColor")\
  .select("Description").show(3, False)
```
```
+----------------------------------+
|Description                       |
+----------------------------------+
|WHITE HANGING HEART T-LIGHT HOLDER|
|WHITE METAL LANTERN               |
|RED WOOLLY HOTTIE WHITE HEART.    |
+----------------------------------+
```

- `locate` : 문자열 위치(1부터 시작)를 정수로 반환
```python
from pyspark.sql.functions import expr, locate

simpleColors = ["black", "white", "red", "green", "blue"]
def color_locator(column, color_string):
  return locate(color_string.upper(), column)\
          .cast("boolean")\
          .alias("is_" + color_string)
selectedColumns = [color_locator(df.Description, c) for c in simpleColors]
selectedColumns.append(expr("*")) # Column 타입

df.select(*selectedColumns).where(expr("is_white OR is_red"))\
  .select("Description").show(3, False)
```
```
+----------------------------------+
|Description                       |
+----------------------------------+
|WHITE HANGING HEART T-LIGHT HOLDER|
|WHITE METAL LANTERN               |
|RED WOOLLY HOTTIE WHITE HEART.    |
+----------------------------------+
```


### E. Timestamp 데이터 타입

- Spark에서 다루는 날짜 데이터 타입
	- date : 달력 형태
	- timestamp : 날짜 + 시간
- Spark의 TimestampType 클래스는 초단위 정밀도까지만 지원
	- 그 이상의 정밀도를 다루려며 데이터를 변환해 처리하는 우회 정책을 사용해야 함.
```python
from pyspark.sql.functions import current_date, current_timestamp

dateDF = spark.range(10)\
  .withColumn("today", current_date())\
  .withColumn("now", current_timestamp())
dateDF.createOrReplaceTempView("dateTable")

dateDF.show(3)
```
```
+---+----------+--------------------+
| id|     today|                 now|
+---+----------+--------------------+
|  0|2024-05-06|2024-05-06 03:34:...|
|  1|2024-05-06|2024-05-06 03:34:...|
|  2|2024-05-06|2024-05-06 03:34:...|
+---+----------+--------------------+
```

- `date_sub`, `date_add` : 이전, 이후 날짜 계산
```python
from pyspark.sql.functions import date_add, date_sub

dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1)
```
```
+------------------+------------------+
|date_sub(today, 5)|date_add(today, 5)|
+------------------+------------------+
|        2024-05-01|        2024-05-11|
+------------------+------------------+
```

- `datediff`, `months_between` : 일 수, 월 수 차이 계산
```python
from pyspark.sql.functions import datediff, months_between, to_date

dateDF.withColumn("week_ago", date_sub(col("today"), 7))\
  .select(datediff(col("week_ago"), col("today"))).show(1)

dateDF.select(
    to_date(lit("2016-01-01")).alias("start"),
    to_date(lit("2017-05-22")).alias("end"))\
  .select(months_between(col("start"), col("end"))).show(1)
```
```
+-------------------------+
|datediff(week_ago, today)|
+-------------------------+
|                       -7|
+-------------------------+

+--------------------------------+
|months_between(start, end, true)|
+--------------------------------+
|                    -16.67741935|
+--------------------------------+
```

- `to_date` : 문자열을 날짜 형태로 변환
```python
from pyspark.sql.functions import to_date, lit

spark.range(5).withColumn("date", lit("2017-01-01"))\
  .select(to_date(col("date"))).show(1)
```
```
+-------------+
|to_date(date)|
+-------------+
|   2017-01-01|
+-------------+
```

- `to_timestamp` : 타임스탬프를 특정한 포맷으로 지정
```python
from pyspark.sql.functions import to_timestamp
cleanDateDF.select(to_timestamp(col("date"), dateFormat)).show()
```
```
+------------------------------+
|to_timestamp(date, yyyy-dd-MM)|
+------------------------------+
|           2017-11-12 00:00:00|
+------------------------------+
```







