---
tags:
  - "#Spark"
created: 2024-05-05
---


## 1. 구조적 API

### A. DataFrame vs Dataset

- 공통
	- 구조화된 컬랙션 개념(지연 연산의 실행 계획, 불변성을 가짐)
	- 스키마를 통해 컬럼명과 데이터 타입을 정의
- 차이
	- Dataset은 스키마에 명시된 데이터 타입의 일치 여부를 "컴파일"에서 확인 => 정적
		- Dataset은 Java, Scalar만 지원
	- DataFrame은 스키마에 명시된 데이터 타입 일치 여부를 "런타임"에서 확인

### B. 구조적 API 실행 과정

1. DataFrame/Dataset/SQL 등으로 코드 작성
2. 논리적 실행 계획으로 변환
	- 추상적인 트랜스 포메이션만 표현(드라이버, 익스큐터 정보 고려 X)
	- 코드의 유효성과 테이블의 컬럼 존재 여부만을 판단
	- 유효성 검증 이후 카탈리스트 옵티마이저로 최적화
3. 물리적 실행 계획으로 변환하면서 최적화
4. 물리적 실행 계획(RDD) 실행


## 2. 구조적 API 기본 연산

### A. 스키마

- DataFrame의 컬럼명과 데이터 타입을 정의
- 일반적으로는 스키마 온 리드가 잘 동작하지만, Long 타입 데이터를 Integer로 인식하는 등의 문제가 발생할 수 있음
- 운영환경에서는 ETL 작업시 스키마를 직접 정의 해주는 것이 필요

```python
# 스키마 정의
from pyspark.sql.types import StructField, StructType, StringType, LongType

myManualSchema = StructType([
  StructField("DEST_COUNTRY_NAME", StringType(), True),
  StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
  StructField("count", LongType(), False, metadata={"hello":"world"})
])
df = spark.read.format("json").schema(myManualSchema)\
  .load("dbfs:/FileStore/shared_uploads/robertmin522@gmail.com/data/flight-data/json/2014_summary-1.json")
```


### B. 컬럼

- 사용자의 표현식으로 DataFrame의 컬럼을 선택, 조작, 제거할 수 있음
- 컬럼의 내용을 조작하려면 DataFrame의 스파크 트랜스포메이션을 활용해야 함
	- `col`, `column` 함수 사용
```python
from pyspark.sql.functions import col, column

col("hello")
column("hello")
```

- DataFrame에서 컬럼을 조작하기 위한 레코드의 여러 값에 대한 트렌스포메이션 집합이 표현식이라 함,
- 컬럼과 컬럼의 트랜스포메이션은 파싱된 표현식과 동일한 논리적 실행 계획으로 컴파일
	- 컬럼은 단지 표현
```python
from pyspark.sql.functions import expr

expr("(((someCol + 5) * 200) - 6) < otherCol")
```


### C. 로우

- 로우는 하나의 레코드를 나타냄
- Row 객체는 스키마 정보를 가지고 있지 않기 때문에 객체를 직접 생성하려면 DataFrame의 스키마와 같은 순서로 값을 명시해야 함.
```python
from pyspark.sql import Row
myRow = Row("Hello", None, 1, False)

# 확인
myRow[0]
```
```
Out[10]: 'Hello'
```


### D. DataFrame 트랜스포메이션

#### DataFrame 생성하기
```python
from pyspark.sql import Row
from pyspark.sql.types import StructField, StructType, StringType, LongType

# 스키마 생성
myManualSchema = StructType([
  StructField("some", StringType(), True),
  StructField("col", StringType(), True),
  StructField("names", LongType(), False)
])

# 로우 값
myRow = Row("Hello", None, 1)
myDf = spark.createDataFrame([myRow], myManualSchema)
myDf.show()
```
```
+-----+----+-----+
| some| col|names|
+-----+----+-----+
|Hello|null|    1|
+-----+----+-----+
```


#### select
- SQL 처럼 데이터 조회
```python
df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)
```
```
+-----------------+-------------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|
+-----------------+-------------------+
|    United States|       Saint Martin|
|    United States|            Romania|
+-----------------+-------------------+
```
- 다양한 컬럼 참조 방식도 사용 가능
```python
from pyspark.sql.functions import expr, col, column
df.select(
    expr("DEST_COUNTRY_NAME"),
    col("DEST_COUNTRY_NAME"),
    column("DEST_COUNTRY_NAME"))\
  .show(2)
```

- select문에서 표현식 사용
	- 아래 `sele`
```python
```