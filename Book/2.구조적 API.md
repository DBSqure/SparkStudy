---
tags:
  - "#Spark"
created: 2024-05-05
---


## 1. 구조적 API

### A. DataFrame vs Dataset

- 공통
	- 구조화된 컬랙션 개념(지연 연산의 실행 계획, 불변성을 가짐)
	- 스키마를 통해 컬럼명과 데이터 타입을 정의
- 차이
	- Dataset은 스키마에 명시된 데이터 타입의 일치 여부를 "컴파일"에서 확인 => 정적
		- Dataset은 Java, Scalar만 지원
	- DataFrame은 스키마에 명시된 데이터 타입 일치 여부를 "런타임"에서 확인

### B. 구조적 API 실행 과정

1. DataFrame/Dataset/SQL 등으로 코드 작성
2. 논리적 실행 계획으로 변환
	- 추상적인 트랜스 포메이션만 표현(드라이버, 익스큐터 정보 고려 X)
	- 코드의 유효성과 테이블의 컬럼 존재 여부만을 판단
	- 유효성 검증 이후 카탈리스트 옵티마이저로 최적화
3. 물리적 실행 계획으로 변환하면서 최적화
4. 물리적 실행 계획(RDD) 실행


## 2. 구조적 API 기본 연산

### A. 스키마

- DataFrame의 컬럼명과 데이터 타입을 정의
- 일반적으로는 스키마 온 리드가 잘 동작하지만, Long 타입 데이터를 Integer로 인식하는 등의 문제가 발생할 수 있음
- 운영환경에서는 ETL 작업시 스키마를 직접 정의 해주는 것이 필요

```python
# 스키마 정의
from pyspark.sql.types import StructField, StructType, StringType, LongType

myManualSchema = StructType([
  StructField("DEST_COUNTRY_NAME", StringType(), True),
  StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
  StructField("count", LongType(), False, metadata={"hello":"world"})
])
df = spark.read.format("json").schema(myManualSchema)\
  .load("dbfs:/FileStore/shared_uploads/robertmin522@gmail.com/data/flight-data/json/2014_summary-1.json")
```


