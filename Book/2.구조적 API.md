---
tags:
  - "#Spark"
created: 2024-05-05
---


## 1. 구조적 API

### A. DataFrame vs Dataset

- 공통
	- 구조화된 컬랙션 개념(지연 연산의 실행 계획, 불변성을 가짐)
	- 스키마를 통해 컬럼명과 데이터 타입을 정의
- 차이
	- Dataset은 스키마에 명시된 데이터 타입의 일치 여부를 "컴파일"에서 확인 => 정적
		- Dataset은 Java, Scalar만 지원
	- DataFrame은 스키마에 명시된 데이터 타입 일치 여부를 "런타임"에서 확인

### B. 구조적 API 실행 과정

1. DataFrame/Dataset/SQL 등으로 코드 작성
2. 논리적 실행 계획으로 변환
	- 추상적인 트랜스 포메이션만 표현(드라이버, 익스큐터 정보 고려 X)
	- 코드의 유효성과 테이블의 컬럼 존재 여부만을 판단
	- 유효성 검증 이후 카탈리스트 옵티마이저로 최적화
3. 물리적 실행 계획으로 변환하면서 최적화
4. 물리적 실행 계획(RDD) 실행


## 2. 구조적 API 기본 연산

### A. 스키마

- DataFrame의 컬럼명과 데이터 타입을 정의
- 일반적으로는 스키마 온 리드가 잘 동작하지만, Long 타입 데이터를 Integer로 인식하는 등의 문제가 발생할 수 있음
- 운영환경에서는 ETL 작업시 스키마를 직접 정의 해주는 것이 필요

```python
# 스키마 정의
from pyspark.sql.types import StructField, StructType, StringType, LongType

myManualSchema = StructType([
  StructField("DEST_COUNTRY_NAME", StringType(), True),
  StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
  StructField("count", LongType(), False, metadata={"hello":"world"})
])
df = spark.read.format("json").schema(myManualSchema)\
  .load("dbfs:/FileStore/shared_uploads/robertmin522@gmail.com/data/flight-data/json/2014_summary-1.json")
```


### B. 컬럼

- 사용자의 표현식으로 DataFrame의 컬럼을 선택, 조작, 제거할 수 있음
- 컬럼의 내용을 조작하려면 DataFrame의 스파크 트랜스포메이션을 활용해야 함
	- `col`, `column` 함수 사용
```python
from pyspark.sql.functions import col, column

col("hello")
column("hello")
```

- DataFrame에서 컬럼을 조작하기 위한 레코드의 여러 값에 대한 트렌스포메이션 집합이 표현식이라 함,
- 컬럼과 컬럼의 트랜스포메이션은 파싱된 표현식과 동일한 논리적 실행 계획으로 컴파일
	- 컬럼은 단지 표현
```python
from pyspark.sql.functions import expr

expr("(((someCol + 5) * 200) - 6) < otherCol")
```


### C. 로우

- 로우는 하나의 레코드를 나타냄
- Row 객체는 스키마 정보를 가지고 있지 않기 때문에 객체를 직접 생성하려면 DataFrame의 스키마와 같은 순서로 값을 명시해야 함.
```python
from pyspark.sql import Row
myRow = Row("Hello", None, 1, False)

# 확인
myRow[0]
```
```
Out[10]: 'Hello'
```


### D. DataFrame 트랜스포메이션

#### DataFrame 생성하기
```python
from pyspark.sql import Row
from pyspark.sql.types import StructField, StructType, StringT회

```python
df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)
```
```
+-----------------+-------------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|
+-----------------+-------------------+
|    United States|       Saint Martin|
|    United States|            Romania|
+-----------------+-------------------+
```
- 다양한 컬럼 참조 방식도 사용 가능
```python
from pyspark.sql.functions import expr, col, column
df.select(
    expr("DEST_COUNTRY_NAME"),
    col("DEST_COUNTRY_NAME"),
    column("DEST_COUNTRY_NAME"))\
  .show(2)
```

- select문에서 표현식 사용
	- 아래 `selectExpr` 사용하면 효율적으로 코드 작성 가능
```python
df.select(expr("DEST_COUNTRY_NAME as destination").alias("DEST_COUNTRY_NAME"))\
  .show(2)
```

- selectExpr
	- 복잡한 표현식을 간단하게 만드는 도구
```python
df.selectExpr(
  "*", # all original columns
  "(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry")\
  .show(2)
```
```
+-----------------+-------------------+-----+-------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|
+-----------------+-------------------+-----+-------------+
|    United States|       Saint Martin|    1|        false|
|    United States|            Romania|   12|        false|
+-----------------+-------------------+-----+-------------+
```

- 집계함수로 가능
```python
df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2)
```

#### withColumn : 컬럼 추가

```python
# select 구문에서 리터널 사용해서 컬럼 추가
df.select(expr("*"), lit(1).alias("numberOne")).show(2)

# withColumn 사용
df.withColumn("numberOne", lit(1)).show(2)
```
```
+-----------------+-------------------+-----+---------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|
+-----------------+-------------------+-----+---------+
|    United States|       Saint Martin|    1|        1|
|    United States|            Romania|   12|        1|
+-----------------+-------------------+-----+---------+
```

- 표현식을 활용해 아래와 같이 컬럼 추가도 가능
```python
df.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")).show(2)
```
```
+-----------------+-------------------+-----+-------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|
+-----------------+-------------------+-----+-------------+
|    United States|       Saint Martin|    1|        false|
|    United States|            Romania|   12|        false|
+-----------------+-------------------+-----+-------------+
```

#### withColumnRenamed : 컬럼명 변경

```python
df.withColumnRenamed("DEST_COUNTRY_NAME", "dest").columns
```
```
Out[25]: ['dest', 'ORIGIN_COUNTRY_NAME', 'count']
```


#### 이스케이핑 : 예약문자, 키워드 사용

- 공백이나 하이픈(-) 같은 예약문자는 컬럼명에 사용할 수 없음
- 이를 사용하기 위해서는 이스케이핑 해야 함!!
	- 단 해당 컬럼을 참조하지 않을 경우 사용 가능하긴 함
```python
dfWithLongColName.selectExpr(
    "`This Long Column-Name`",
    "`This Long Column-Name` as `new col`")\
  .show(2)
```
```
+---------------------+------------+
|This Long Column-Name|     new col|
+---------------------+------------+
|         Saint Martin|Saint Martin|
|              Romania|     Romania|
+---------------------+------------+
```


#### drop : 컬럼 제거

```python
df.drop("ORIGIN_COUNTRY_NAME").columns
```


#### cast : 컬럼 타입 변경

```python
df.withColumn("count2", col("count").cast("string")).show(2)
```


#### where, filter : 로우 필터링

```python
df.filter(col("count") < 2).show(2)

df.where("count < 2").show(2)
```
```
+-----------------+-------------------+-----+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
+-----------------+-------------------+-----+
|    United States|       Saint Martin|    1|
|           Malawi|      United States|    1|
+-----------------+-------------------+-----+
```

- 여러 개 할경우 차례대로 필터를 연결하고 실행 순서 판단은 Spark가 하도록 코드 작성
```python
df.where(col("count") < 2).where(col("ORIGIN_COUNTRY_NAME") != "Croatia")\
  .show(2)
```
```
+-----------------+-------------------+-----+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
+-----------------+-------------------+-----+
|    United States|       Saint Martin|    1|
|           Malawi|      United States|    1|
+-----------------+-------------------+-----+
```


#### distinct : 고유한 로우 얻기

- 중복되지 않은 값을 얻는 연산
```python
df.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct().count()
```








