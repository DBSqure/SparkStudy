---
tags:
  - "#Spark"
created: 2024-05-06
---


## 1. CSV 파일

### A. Read

- CSV 파일을 읽기 위해 DataFrameReader 객체 생성
- 스파크는 지연 연산되기 때문에 잡 실행 시점에만 오류가 발생
```python
csvFile = spark.read.format("csv")\
    .option("header", "true")\
    .option("mode", "FAILFAST")\
    .option("inferSchema", "true")\
    .load("dbfs:/FileStore/shared_uploads/robertmin522@gmail.com/data/flight-data/csv/2010_summary-1.csv")
```

### B. Write

- 작성한 `my-tsv-file` 은 여러 파일이 들어있는 디렉토리 형태
- 실제로 데이터를 쓰는 시점에 DF 파티션 수를 반영하여 파일이 생성
```python
csvFile.write.format("csv").mode("overwrite").option("sep", "\t")\
  .save("/tmp/my-tsv-file.tsv")
```



## 2. Json 파일

### A. Read

```python
spark.read.format("json")\
    .option("mode", "FAILFAST")\
    .option("inferSchema", "true")\
    .load("dbfs:/FileStore/shared_uploads/robertmin522@gmail.com/data/flight-data/json/2010_summary-2.json").show(5)
```

### B. Write

```python
csvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json")
```


## 3. Parquet 파일

- 컬럼 기반의 데이터 저장 방식
- 분석 워크로드에 최적화
- 개별 컬럼으로 읽을 수 있고 컬럼 기반의 압축 기능 제공

### A. Read

```python
spark.read.format("parquet")\
  .load("/data/flight-data/parquet/2010-summary.parquet").show(5)
```

### B. Write

```python
csvFile.write.format("parquet").mode("overwrite")\
  .save("/tmp/my-parquet-file.parquet")
```


## 4. ORC 파일

- 하둡 워크로드를 위해 설계된 컬럼 기반의 파일 포맷
- 대규모 스트리밍 읽기에 최적화
- 로우를 신속하게 찾아낼 수 있는 기능 제공
- ORC는 하이브에 최적화, Parquet는 스파크에 최적화

### A. Read

```python
spark.read.format("orc").load("/data/flight-data/orc/2010-summary.orc").show(5)
```

### B. Write

```python
csvFile.write.format("orc").mode("overwrite").save("/tmp/my-orc-file.orc")
```


## 5. SQL


### A. Connection

```python
pgDF = spark.read.format("jdbc")\
  .option("driver", "org.postgresql.Driver")\
  .option("url", "jdbc:postgresql://database_server")\
  .option("dbtable", "schema.tablename")\
  .option("user", "username").option("password", "my-secret-password").load()
```

### B. Read

```python
pushdownQuery = """(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info)
  AS flight_info"""
dbDataFrame = spark.read.format("jdbc")\
  .option("url", url).option("dbtable", pushdownQuery).option("driver",  driver)\
  .load()

```

### C. 병렬로 Read

- `getNumPartitions` 으로 읽기 및 쓰기 용 동시 작업 수를 제한할 수 있는 최대 파티션 수를 설정
```python
props = {"driver":"org.sqlite.JDBC"}
predicates = [
  "DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'",
  "DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'"]
spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()
spark.read.jdbc(url,tablename,predicates=predicates,properties=props)\
  .rdd.getNumPartitions() # 2

colName = "count"
lowerBound = 0L
upperBound = 348113L # this is the max count in our database
numPartitions = 10

spark.read.jdbc(url, tablename, column=colName, properties=props,
                lowerBound=lowerBound, upperBound=upperBound,
                numPartitions=numPartitions).count() # 255
```

- 주의!! 연관성 없는 조건절을 정의하면 중복 로우가 발생할 수 있음
```python
props = {"driver":"org.sqlite.JDBC"}
predicates = [
  "DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'",
  "DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'"]
spark.read.jdbc(url, tablename, predicates=predicates, properties=props).count()
```

### D. Write

```python
newPath = "jdbc:sqlite://tmp/my-sqlite.db"
csvFile.write.jdbc(newPath, tablename, mode="overwrite", properties=props)
```


## 6. 고급 I/O 개념

- 스파크 I/O 최적화

### A. 분할 가능한 파일 타입과 압축 방식

- 특정 파일 포맷은 전체 파일이 아닌 쿼리에 필요한 부분만 읽을 수 있어 성능 향상에 기여
- 추천 : parquet, GZIP압축 방식

### B. 병렬로 데이터 읽기

- 
