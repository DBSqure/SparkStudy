---
tags:
  - "#Spark"
created: 2024-05-06
---


## 1. 집계 연산

### A. 데이터  캐싱

- 파티션을 훨씬 적은 수로 분할할 수 있도록 리파티셔닝한 후 빠르게 접근할 수 있도록 캐싱 수행
- 파티션 수를 줄여서 적은 양의 데이터를 가진 수많은 파일에 접근용이 ?
```python
df = spark.read.format("csv")\
    .option("header", "true")\
    .load("dbfs:/FileStore/shared_uploads/robertmin522@gmail.com/data/retail-data/all/*.csv")

# 캐싱
df.cache()

df.printSchema()
df.createOrReplaceTempView("dfTable")
```


### B. 기본 집계 함수

#### count
```python
from pyspark.sql.functions import count

df.select(count("StockCode")).show() # 541909
```

#### countDistinct : 고유값 수
```python
from pyspark.sql.functions import countDistinct

df.select(countDistinct("StockCode")).show() # 4070
```

#### approx_count_distinct : 개수 근사치
- 데이터가 너무 많을 경우 대략적인 수를 확인
- 최대 추정 오류율을 파라미터로 사용해서 대략적인 결과를 빠르게 확인
```python
from pyspark.sql.functions import approx_count_distinct

df.select(approx_count_distinct("StockCode", 0.1)).show() # 3364
```

#### first, last : 첫 번째 ,마지막 값
```python
from pyspark.sql.functions import first, last

df.select(first("StockCode"), last("StockCode")).show()
```

#### min, max : 최대 최소
```python
from pyspark.sql.functions import min, max

df.select(min("Quantity"), max("Quantity")).show()
```

#### sum : 합
```python
from pyspark.sql.functions import sum

df.select(sum("Quantity")).show() # 5176450
```

#### sumDistinct : 고유값 합
```python
from pyspark.sql.functions import sumDistinct

df.select(sumDistinct("Quantity")).show() # 29310
```

#### avg, mean : 평균
```python
from pyspark.sql.functions import sum, count, avg, expr

df.select(
    count("Quantity").alias("total_transactions"),
    sum("Quantity").alias("total_purchases"),
    avg("Quantity").alias("avg_purchases"),
    expr("mean(Quantity)").alias("mean_purchases"))\
  .selectExpr(
    "total_purchases/total_transactions",
    "avg_purchases",
    "mean_purchases").show()
```
```
+--------------------------------------+----------------+----------------+
|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|
+--------------------------------------+----------------+----------------+
|                      9.55224954743324|9.55224954743324|9.55224954743324|
+--------------------------------------+----------------+----------------+
```

#### 분산, 표준편차
- 표본표준편차 : `variance`, `stddev` 함수 사용
- 모표준편차 : `var_pop`, `stddev_pop` 함수 사용
```python
from pyspark.sql.functions import var_pop, stddev_pop
from pyspark.sql.functions import var_samp, stddev_samp

df.select(var_pop("Quantity"), var_samp("Quantity"),
  stddev_pop("Quantity"), stddev_samp("Quantity")).show()
```

#### 비대칭도(skewness), 첨도(kurtosis)
- 비대칭도 : 데이터 평균의 비대칭 정도를 측정
- 첨도 : 데이터 끝 부분을 측정
- 두 값 모두 확률변수와 확률분포로 데이터를 모델링할 때 중요!!
```python
from pyspark.sql.functions import skewness, kurtosis

df.select(skewness("Quantity"), kurtosis("Quantity")).show()
```
```
+--------------------+------------------+
|  skewness(Quantity)|kurtosis(Quantity)|
+--------------------+------------------+
|-0.26407557610526183|119768.05495536885|
+--------------------+------------------+
```

#### 공분산(covariance), 상관관계(correlation)
- 두 컬럼 사이의 영향도 비교
```python
from pyspark.sql.functions import corr, covar_pop, covar_samp

df.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"),
    covar_pop("InvoiceNo", "Quantity")).show()
```
```
+-------------------------+-------------------------------+------------------------------+
|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|
+-------------------------+-------------------------------+------------------------------+
|     4.912186085628163E-4|             1052.7280543887264|            1052.7260778726222|
+-------------------------+-------------------------------+------------------------------+
```

#### 복합 데이터 타입의 집계
- 특정 컬럼의 값을 리스트로 수집하거나 셋 데이터 타입으로 고윳값을 수집
```python
from pyspark.sql.functions import collect_set, collect_list

df.agg(collect_set("Country"), collect_list("Country")).show()
```
```
+--------------------+---------------------+
|collect_set(Country)|collect_list(Country)|
+--------------------+---------------------+
|[Portugal, Italy,...| [United Kingdom, ...|
+--------------------+---------------------+
```





