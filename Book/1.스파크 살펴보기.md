---
tags:
  - "#Spark"
created: 2024-05-04
---


## 1. Apache Spark 란?

- 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합

### A. Spark 철학

- 통합
	- "빅데이터 애플리케이션 개발에 필요한 통합 플랫포 제공"
- 컴퓨팅 엔진
	- 통합이라는 관점을 중시하여 기능의 범위를 컴퓨팅 엔진으로 제한
	- 데이터 연산하는 역할만 수행하며 영구 저장소 역할을 수행하지 않음
	- 데이터 저장 위치에 상관없이 처리에 집중
- 라이브러리

### B. Spark 등장 배경

- 2005년경 부터 하드웨어 성능 향상에 한계
- 단일 프로세스 성능을 향상시키는 방법 대신 모든 코어가 같은 속도로 동작하는 CPU 코어를 추가하는 방향으로 전환 => 병렬 처리
- 기존의 데이터 처리하는 방식이 아닌 새로운 프로그래밍 모델이 필요 => Spark 탄생


### C. Spark 역사

- 2009년 스파크 연구 프로젝트에서 시작
- 하둡 맵리듀스를 사용하여 데이터를 병렬 처리하는 과정에서 문제점을 확인
	- 맵리듀스를 활용하기 사용하는 대규모 어플리케이션의 난이도와 효율성 문제
	- 맵리듀스 처리를 위해 단계적으로 잡을 개발하고 클러스터에서 각각 실행하는 데 어려움
	- 매번 실행 때마다 데이터를 처음부터 읽어야 하는 문제가 있음
- 위의 문제 해결
	- 연산 단계 사이에서 메모리에 저장된 데이터를 효율적으로 공유할 수 있는 새로운 엔진 기반 API 구현
	- 초기 버전 : 함수형 연산(맵리듀스 같은 병렬 연산을 수행하는 API)
	- ver 1.0 : Spark SQL(구조화된 데이터를 기반으로 동작하는 API)
	- 현재 : DataFrame, 머신러닝 파이프라인, 구조적 스트리밍 등 API 제공


## 2. Spark Architecture

- 스파크는 클러스터의 데이터 처리 작업을 관리하고 조율
- 사용자 => 클러스터 매니저에 제출
- 클러스터 매니저는 애플리케이션 실행에 필요한 자원을 할당하며, 우리는 할당받은 자원으로 작업을 처리

### A. Spark Application

- 드라이버(driver) 프로세스, 익스큐터(executor) 프로세스로 구성
- 드라이버 프로세스 `SparkSession`
	- 클러스터 노드 중 하나에 실행하며 `main()` 함수를 실행
	- 스파크 애플리케이션 유지 관리, 사용자 입력에 대한 응답, 배포, 스케줄링 역할 등
	- 애플리케이션의 수명 주기 동안 관련 정보 모두 유지
- 익스큐터 프로세스
	- 드라이버 프로세스가 할당한 작업을 수행
- `SparkSession`  객체를 진입점으로 사용하여 사용자가 다른 언어로 작성하더라도 JVM에서 실행할 수 있는 코드로 변환
- Spark API
	- 2가지 API를 제공
	- 저수준의 비구조적 API
	- 고수준의 구조적 API


### B. DataFrame

- 테이블의 데이터를 row, column으로 단순하게 표현한 구조적 API


```python
df = spark.range(1000).toDF("number")
```

```sh
Out[2]: DataFrame[number: bigint]
```

- 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션으로 데이터를 분리
	- 파티션이 하나면 스파크에 여러 이스큐터가 있어도 병렬성 = 1
	- 파티션이 여러개라도 이스큐터가 하나라면 병렬성 = 1


### C. 트랜스포메이션 & 액션

#### 트랜스포메이션

- 스파크의 핵심 데이터 구조 "불변성"을 가짐
- DataFrame을 변경하려면 변경 방법을 Spark에 전달 => 트렌스포메이션
- Lazy Evaluation
	- 연산 그래프를 처리하기 직전까지 기다리는 동작을 의미
	- 즉, 연산 명령이 내려진 즉시 처리하는 것이 아닌 원시 데이터에 적용할 트랜스포메이션의 실행 계획을 생성
	- 스파크 코드를 실행하는 마지막 순간까지 대기하다가 위에 계획한 실행 계획을 컴파일 진행

#### 액션

- 앞서 트랜스포메이션으로 논리적 실행 계획을 세운 후 실제 동작을 수행하기 위해 액션 명령을 내려야 함
- 액션을 지정하면 스파크 잡이 시작

```python
ex1 = df.where("number % 2 = 0")  # 좁은 트랜스포메이션 : 필터
ex1.count()  # 넓은 트랜스포메이션 : 파티션 별로 레코드 수를 카운트
```



## 3. Spark 간단한 실행 예제

### A. Read-Sort-Take, 파티션 수 설정

#### Read

```python
# 데이터 읽기
df1 = spark.read.format("csv").option("header", "true").load("dbfs:/FileStore/shared_uploads/robertmin522@gmail.com/data/flight-data/2012_summary.csv")

# 데이터 확인
df1.take(3)
```

```
Out[9]: [Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count='1'), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count='252'), Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count='13')]
```

#### Sort

```python
# 실행 계획 확인
df1.sort("count").explain()
```

- Sort > Exchnge > FileScan 순서로 진행
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Sort [count#182 ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning(count#182 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=288]
      +- FileScan csv [DEST_COUNTRY_NAME#180,ORIGIN_COUNTRY_NAME#181,count#182] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/shared_uploads/robertmin522@gmail.com/data/flight-data..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:string>
```

#### 파티션 설정

- 파티션 파라미터를 설정하여 물리적 실행 특성을 제어
```python
# 파티션 설정
spark.conf.set("spark.sql.shuffle.partitions", "5")

df1.sort("count").take(2)
```

```
Out[11]: [Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count='1'),
 Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Togo', count='1')]
```



